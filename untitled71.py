# -*- coding: utf-8 -*-
"""Untitled71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ouZTHa2L_s75EQAqtE6x5P1lMTtMZ_nj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('creditcard.csv')
df.head()

df.shape

df.dtypes

df.isnull().sum()

df['Class'].value_counts(normalize=True)*100

sns.histplot(data=df,x='Amount',hue='Class')

"""Preprocesing and Sampling"""

y = df['Class']
X = df.drop('Class', axis=1)

# Drop rows where the target variable 'Class' is NaN
df.dropna(subset=['Class'], inplace=True)

# Re-define X and y after dropping rows
y = df['Class']
X = df.drop('Class', axis=1)

# Re-scale Amount and Time for the updated X dataframe
from sklearn.preprocessing import StandardScaler

# 1. Create the scaler
scaler = StandardScaler()

# 2. Fit and transform Amount and Time
X_scaled = scaler.fit_transform(X[['Amount', 'Time']])

# 3. Replace the original columns in X with the scaled values
X[['Amount', 'Time']] = X_scaled

# Now proceed with the train_test_split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from imblearn.over_sampling import SMOTE
# 1. Create the SMOTE object
smote = SMOTE(random_state=42)

# 2. Apply it to the training data
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

pd.Series(y_train_resampled).value_counts()

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train_resampled, y_train_resampled)

from sklearn.metrics import accuracy_score, f1_score, recall_score,classification_report,confusion_matrix
y_pred = model.predict(X_test)
accuracy_score(y_test, y_pred)
f1_score(y_test, y_pred)
recall_score(y_test, y_pred)
print(classification_report(y_test, y_pred))

from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train_resampled, y_train_resampled)
y_pred = model.predict(X_test)
accuracy_score(y_test, y_pred)
f1_score(y_test, y_pred)
recall_score(y_test, y_pred)
print(classification_report(y_test, y_pred))

import shap

# 1. Create the explainer (assign to variable called `explainer`)
explainer = shap.TreeExplainer(model)

# 2. Get SHAP values from the explainer
shap_values = explainer.shap_values(X_test)

# 3. Plot the summary
shap.summary_plot(shap_values, X_test)

#Get fraud probabilities
fraud_probs = model.predict_proba(X_test)[:, 1]
results_df = X_test.copy()
results_df['Actual'] = y_test.values
results_df['Predicted'] = y_pred
results_df['Fraud_Probability'] = fraud_probs
results_df.to_csv('results.csv', index=False)

